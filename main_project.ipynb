{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported modules from eval.\n"
     ]
    }
   ],
   "source": [
    "### import the evaluation metrics from the helper open-instruct project ###\n",
    "eval_dir = r\"D:\\University of Illinois Chicago\\Classes\\CS421\\GradResearch\\open-instruct\"\n",
    "if eval_dir not in sys.path:\n",
    "    sys.path.append(eval_dir)\n",
    "\n",
    "from eval.utils import load_hf_lm_and_tokenizer, generate_completions\n",
    "from eval.templates import create_prompt_with_tulu_chat_format\n",
    "\n",
    "print(\"Successfully imported modules from eval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialise logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize OpenAI client\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found! Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sni_json(task_name):\n",
    "    \"\"\"Fetches task data from the Natural Instructions dataset and returns a DataFrame.\"\"\"\n",
    "    url = f\"https://raw.githubusercontent.com/allenai/natural-instructions/master/tasks/{task_name}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data[\"Instances\"])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching data for task {task_name}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sni_json_wurl(url):\n",
    "    \"\"\"Fetches data from a given URL and returns a DataFrame.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data[\"Instances\"])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching data from URL {url}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Searching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dataframe_sw(df, query):\n",
    "    \"\"\"Filters rows where 'input' starts with the query string.\"\"\"\n",
    "    mask = df['input'].str.startswith(query)\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dataframe_co(df, query):\n",
    "    \"\"\"Filters rows where 'input' contains the query string.\"\"\"\n",
    "    mask = df['input'].str.contains(query, regex=False)\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Reads a JSON file and returns the data.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading JSON file {file_path}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(s, N):\n",
    "    \"\"\"Parses the answer for task N from the given string.\"\"\"\n",
    "    pattern = f'<task{N}>(.*?)<task{N}/>'\n",
    "    match = re.search(pattern, str(s), re.DOTALL)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_text(text):\n",
    "    \"\"\"Normalizes text by attempting to evaluate it if it's numeric.\"\"\"\n",
    "    if \"/\" in text or text.startswith(\"(\"):\n",
    "        return text\n",
    "    try:\n",
    "        # Safely evaluate numeric expressions\n",
    "        return str(eval(text, {\"__builtins__\": {}}, {}))\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(answer, prediction):\n",
    "    \"\"\"Calculates step-wise and conditional accuracies.\"\"\"\n",
    "    if len(answer) != len(prediction):\n",
    "        raise ValueError(\"Answer and prediction must have the same length.\")\n",
    "    step_accuracies = np.array([int(a == p) for a, p in zip(answer, prediction)])\n",
    "    conditional_accuracies = np.cumprod(step_accuracies)\n",
    "    return step_accuracies, conditional_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(df, data, tuid):\n",
    "    \"\"\"Calculates average accuracies over the DataFrame.\"\"\"\n",
    "    num_answers = len(next(iter(data[tuid][\"instance\"].values()))[\"answers\"])\n",
    "    total_step_accuracies = np.zeros(num_answers)\n",
    "    total_conditional_accuracies = np.zeros(num_answers)\n",
    "    for _, row in df.iterrows():\n",
    "        instance_data = data[tuid][\"instance\"][row.uid]\n",
    "        answers = [str(ans) for ans in instance_data[\"answers\"]]\n",
    "        predictions = [\n",
    "            str(norm_text(parse_answer(row.generation, idx+1) or \"\"))\n",
    "            for idx in range(len(answers))\n",
    "        ]\n",
    "        step_accuracies, conditional_accuracies = calculate_accuracy(answers, predictions)\n",
    "        total_step_accuracies += step_accuracies\n",
    "        total_conditional_accuracies += conditional_accuracies\n",
    "    n = len(df)\n",
    "    return total_step_accuracies / n, total_conditional_accuracies / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Reconstruction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_instruction(data, num, full=True):\n",
    "    \"\"\"Reconstructs instruction text for GPT input.\"\"\"\n",
    "    if num == -1:\n",
    "        num = len(data['instruction'])\n",
    "    query = data.get('query', '')\n",
    "    instructions = data.get('instruction', {})\n",
    "    contexts = data.get('context', {})\n",
    "    if full:\n",
    "        filtered_instructions = '\\n'.join(\n",
    "            f\"#{i}: {instructions.get(str(i), '')}\" for i in range(1, num+1)\n",
    "        )\n",
    "        filtered_contexts = '\\n'.join(\n",
    "            v for i in range(1, num+1) for k, v in contexts.items() if k.startswith(str(i))\n",
    "        )\n",
    "        final_string = f\"{query}\\n{filtered_instructions}\\n\\n{filtered_contexts}\"\n",
    "    else:\n",
    "        filtered_instructions = instructions.get(str(num), '')\n",
    "        filtered_contexts = '\\n'.join(\n",
    "            v for k, v in contexts.items() if k.startswith(str(num))\n",
    "        )\n",
    "        final_string = f\"{filtered_instructions}\\n\\n{filtered_contexts}\"\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Interaction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(instance, model):\n",
    "    \"\"\"Calls the OpenAI API with the provided instance and model.\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": instance}],\n",
    "            stop=\"### Task:\",\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_answer: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_multithread(gpt_input_list, model):\n",
    "    \"\"\"Executes GPT calls concurrently and returns the responses.\"\"\"\n",
    "    MAX_RETRIES = 3\n",
    "    answers = []\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_answer, gpt_input, model)\n",
    "                    for gpt_input in gpt_input_list[len(answers):]\n",
    "                ]\n",
    "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                    answers.append(future.result())\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during GPT calls: {e}\")\n",
    "            time.sleep(10)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instances(idx_list, data, CoT, tuid):\n",
    "    \"\"\"Generates input instances for GPT.\"\"\"\n",
    "    input_list = []\n",
    "    for uid in idx_list:\n",
    "        instance_data = data[tuid][\"instance\"][uid]\n",
    "        instruction = reconstruct_instruction(instance_data, -1)\n",
    "        full_input = f\"### Example:\\n\\n{CoT}\\n\\n### Task:\\n\\n{instruction}\"\n",
    "        input_list.append(full_input)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(list1, list2):\n",
    "    \"\"\"Calculates the fraction of matching elements between two lists.\"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must have the same length.\")\n",
    "    return sum(a == b for a, b in zip(list1, list2)) / len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_ef(list1, list2):\n",
    "    \"\"\"Calculates similarity considering positions where at least one value is True.\"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must have the same length.\")\n",
    "    total_positions = sum(1 for a, b in zip(list1, list2) if a or b)\n",
    "    matched_positions = sum(a == b for a, b in zip(list1, list2) if a or b)\n",
    "    return matched_positions / total_positions if total_positions else 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_batch_instruction(data):\n",
    "    \"\"\"Constructs batch instructions by pairing data entries.\"\"\"\n",
    "    instruction_lists = {1: [], 2: [], 3: []}\n",
    "    keys = list(data.keys())\n",
    "    paired_keys = [(keys[i], keys[i+1]) for i in range(0, len(keys)-1, 2)]\n",
    "    for uid_0, uid_1 in paired_keys:\n",
    "        for num in [1, 2, 3]:\n",
    "            uid_0_instruction = reconstruct_instruction(data[uid_0], num, full=False)\n",
    "            uid_1_instruction = reconstruct_instruction(data[uid_1], num, full=False)\n",
    "            uid_1_instruction = uid_1_instruction.replace(\n",
    "                \"<task1>\", f\"<task{num}>\"\n",
    "            ).replace(\"<task1/>\", f\"<task{num}/>\")\n",
    "            final_instruction = (\n",
    "                f\"Read the following passage, and solve both of the instructions provided.\\n\"\n",
    "                f\"### Instruction 1: {uid_0_instruction}\\n\\n\"\n",
    "                f\"### Instruction 2: {uid_1_instruction}\"\n",
    "            )\n",
    "            instruction_lists[num].append(final_instruction)\n",
    "    return instruction_lists, paired_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_score(df, data, tuid, return_id=False):\n",
    "    \"\"\"Calculates accuracy scores for batched tasks.\"\"\"\n",
    "    df = df.dropna()\n",
    "    s1_score = 0\n",
    "    s2_score = 0\n",
    "    uid_list = []\n",
    "    for uid_str, generation in df[[\"uid\", \"generation\"]].values:\n",
    "        uid = eval(uid_str)\n",
    "        p1 = str(norm_text(parse_answer(generation, 1) or \"\"))\n",
    "        p2 = str(norm_text(parse_answer(generation, 2) or \"\"))\n",
    "        a1 = str(data[tuid][\"instance\"][uid[0]][\"answers\"][0])\n",
    "        a2 = str(data[tuid][\"instance\"][uid[1]][\"answers\"][0])\n",
    "        if a1 == p1:\n",
    "            s1_score += 1\n",
    "            uid_list.append(uid[0])\n",
    "        if a2 == p2:\n",
    "            s2_score += 1\n",
    "            uid_list.append(uid[1])\n",
    "    n = len(df)\n",
    "    if return_id:\n",
    "        return uid_list\n",
    "    return s1_score / n, s2_score / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STI functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generations(task_id, model_name, generations, step):\n",
    "    \"\"\"\n",
    "    Saves the generated outputs to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(generations)\n",
    "    filename = f\"data/{model_name}-MTI-{task_id}-s{step}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved step {step} generations to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(cot_sample, s1, s2, s3, instruction1, instruction2, instruction3, gen1, gen2, step):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt for the given step.\n",
    "    \"\"\"\n",
    "    # Example section\n",
    "    example_section = f\"### Example:\\n\\n### Instruction: {cot_sample}\\n\\n\"\n",
    "\n",
    "    # Task section\n",
    "    task_section = \"### Task:\\n\\n\"\n",
    "    task_section += f\"### (1) Instruction: {instruction1}\\n\\n\"\n",
    "    if step >= 2:\n",
    "        task_section += f\"### Answer:\\n\\n{gen1}\\n\\n\"\n",
    "        task_section += f\"### (2) Instruction: {instruction2}\\n\\n\"\n",
    "    if step == 3:\n",
    "        task_section += f\"### Answer:\\n\\n{gen2}\\n\\n\"\n",
    "        task_section += f\"### (3) Instruction: {instruction3}\\n\\n\"\n",
    "\n",
    "    # Answer section\n",
    "    task_section += \"### Answer:\\n\\n\"\n",
    "\n",
    "    # Full prompt\n",
    "    full_prompt = example_section + task_section\n",
    "    return create_prompt_with_tulu_chat_format([{\"role\": \"user\", \"content\": full_prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_outputs(model, tokenizer, task_data, cot_sample, s1, batch_size, step, previous_generations=None, s2=None, s3=None):\n",
    "    \"\"\"\n",
    "    Generates outputs for a specific step.\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    instances = task_data[\"instance\"]\n",
    "\n",
    "    # For each instance in the task\n",
    "    for idx, (uid, instance) in enumerate(instances.items()):\n",
    "        # Reconstruct instructions for the current and previous steps\n",
    "        instruction1 = reconstruct_instruction(instance, 1, False)\n",
    "        instruction2 = reconstruct_instruction(instance, 2, False) if step >= 2 else None\n",
    "        instruction3 = reconstruct_instruction(instance, 3, False) if step == 3 else None\n",
    "\n",
    "        # Get previous generations if any\n",
    "        gen1 = previous_generations[idx]['generation'] if step >= 2 else None\n",
    "        gen2 = previous_generations[idx]['generation'] if step == 3 else None\n",
    "\n",
    "        # Create the prompt for the current step\n",
    "        prompt = create_prompt(\n",
    "            cot_sample, s1, s2, s3,\n",
    "            instruction1, instruction2, instruction3,\n",
    "            gen1, gen2, step\n",
    "        )\n",
    "        input_list.append(prompt)\n",
    "\n",
    "    # Generate completions\n",
    "    generation_time, generated_texts = generate_completions(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        input_list,\n",
    "        batch_size=batch_size,\n",
    "        stop_id_sequences=None,\n",
    "        add_special_tokens=True,\n",
    "        disable_tqdm=False,\n",
    "        max_new_tokens=2048,\n",
    "        min_new_tokens=32,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=1.0\n",
    "    )\n",
    "\n",
    "    # Package the results\n",
    "    return [\n",
    "        {\n",
    "            \"uid\": uid,\n",
    "            \"generation\": gen_text,\n",
    "            \"generation_time\": generation_time\n",
    "        }\n",
    "        for uid, gen_text in zip(instances.keys(), generated_texts)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_STI():\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Run the script with a specified model and batch size.\")\n",
    "    parser.add_argument(\"--model_name\", type=str, required=True, help=\"Name of the model to load\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, required=True, help=\"Batch size for generation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load the MTI benchmark data\n",
    "    with open(r\"D:\\University of Illinois Chicago\\Classes\\CS421\\GradResearch\\MTI-Bench\\data\\MTI_BENCH.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Load the language model and tokenizer\n",
    "    model, tokenizer = load_hf_lm_and_tokenizer(\n",
    "        args.model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Load the Chain-of-Thought (CoT) breakdown data\n",
    "    cot_df = pd.read_excel(r\"D:\\University of Illinois Chicago\\Classes\\CS421\\GradResearch\\MTI-Bench\\data\\cot_breakdown.xlsx\")\n",
    "\n",
    "    print(\"Starting evaluation\")\n",
    "\n",
    "    # Iterate over each task in the data\n",
    "    for task_id, task_data in data.items():\n",
    "        print(f\"Processing task {task_id}\")\n",
    "\n",
    "        # Get the sample CoT steps for the current task\n",
    "        cot_row = cot_df[cot_df[\"tuid\"] == int(task_id)]\n",
    "        if cot_row.empty:\n",
    "            print(f\"No CoT data found for task {task_id}\")\n",
    "            continue\n",
    "        _, s1, s2, s3 = cot_row.values[0]\n",
    "\n",
    "        # Get the sample CoT from the task data\n",
    "        cot_sample = task_data.get(\"sample\", \"\")\n",
    "\n",
    "        # Generate outputs for each step\n",
    "        generated_s1 = generate_step_outputs(\n",
    "            model, tokenizer, task_data, cot_sample, s1, args.batch_size, step=1\n",
    "        )\n",
    "        save_generations(task_id, args.model_name, generated_s1, step=1)\n",
    "\n",
    "        generated_s2 = generate_step_outputs(\n",
    "            model, tokenizer, task_data, cot_sample, s1, args.batch_size, step=2,\n",
    "            previous_generations=generated_s1, s2=s2\n",
    "        )\n",
    "        save_generations(task_id, args.model_name, generated_s2, step=2)\n",
    "\n",
    "        generated_s3 = generate_step_outputs(\n",
    "            model, tokenizer, task_data, cot_sample, s1, args.batch_size, step=3,\n",
    "            previous_generations=(generated_s1, generated_s2), s2=s2, s3=s3\n",
    "        )\n",
    "        save_generations(task_id, args.model_name, generated_s3, step=3)\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del generated_s1, generated_s2, generated_s3\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model_name MODEL_NAME --batch_size\n",
      "                             BATCH_SIZE\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model_name, --batch_size\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manan\\anaconda3\\envs\\CS421\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# run the main sti function\n",
    "main_STI()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
